Adaptation research:

# TODO: Questions to be answered
# TODO: Q1) Can you learn good model from random policy (how does it generalize to state distribution induced by trained policy)
# TODO: Q2) Does confidence/uncertainty estimate using dropout or ensembles work
# TODO: Q3) Does RNN learn model for adapting param (supervised learning)

# Paper extract:
SNAIL:
- Gated activation functions: Might be usefull to allow or block output through
- Carrying over state to next episode apparently crucial. Needs more understanding
- Use of causal temporal convolutions alongside k,q,v attention
- Neural network architecture apparently important (skip connections, etc)
- Use larger batchsize (50k timesteps)

Learning VTOL policies:
- Integrated error input (similar as in PID) might be useful

RL^2:
- Apparently inter-episode hidden states matter

What matters in deep RL:
- Use the PPO policy loss. Start with the clipping threshold set to 0.25 but also try
lower and higher values if possible
- Initialize the last policy layer with 100× smaller weights
- Use tanh both as the activation function (if the networks are not too deep) and to transform the samples from the normal
distribution to the bounded action space. Use a wide value MLP (no layers shared with the policy)
but tune the policy width (it might need to be narrower than the value MLP).
- Recommendation. Use GAE with λ = 0.9 but neither Huber loss nor PPO-style value loss clipping.
- Discount factor γ is one of the most important hyperparameters and should be tuned per environment (start with γ = 0.99)
- Adam [8] optimizer with momentum β1 = 0.9 and a tuned learning rate (0.0003 is a safe default).
Linearly decaying the learning rate may slightly improve performance but is of secondary importance.
- One of the surprising insights is that the initial action distribution plays an important role in agent performance.

# Planned adaptation algos:
- Plain training using RNN (supervised model learning and RL policy)
- Oracle with params as inputs
- Using parameter predictor
- SNAIL - like architecture

# TODO:

# -----------
# TODO: Currently integrating conv policy
# TODO: Somethign wrong with quad env, not training
# TODO: Find out how variance is handled on A2C SB
# TODO: THROW QUAD AND HEXA IN THE TUNER
# TODO: Check what can be run on GRID, perhaps it can be used for hyperparam opt
# -----------




# On RCI cluster:
ml Python/3.7.4-GCCcore-8.3.0 # To load python

# Python installs:
pip install --user optuna
pip install --user stable_baselines3
pip install --user opensimplex
pip install --user pybullet

# Obstacle opt:
Trial 15 finished with value: 30.30074812974557 and parameters: {'learnin
g_rate': 0.0021304918967999047, 'gamma': 0.9827792852302045, 'ou_sigma': 0.5170484518188396, 'jrl_fem
ur': -0.8536290252223969, 'jrl_tibia': -0.602409390012748, 'jr_diff_femur': 1.3874365117907883, 'jr_d
iff_tibia': 1.4985882639221828}.

# Quad opt:
Trial 97 finished with value: 323.91413818539365 and parameters: {'learning_rate': 0.0012294349411285863, 'ou_sigma': 0.2352080103675635, 'gamma': 0.98211589599389}.

# Eef
Trial 13 finished with value: 54.62433840897107 and parameters: {'learning_rate': 0.0036451235136335494, 'gamma': 0.966225662839895, 'phase_scalar': 0.1755193229832495, 'z_aux_scalar': 0.038643589498729036, 'x_mult_scalar': 0.01992210735192025, 'ou_sigma': 0.572476478551845}. Best is trial 13 with value: 54.62433840897107.

# Rugged:
Trial 21 finished with value: 261.1324298132091 and parameters:
{'learning_rate': 0.0022925056627022995, 'gamma': 0.9739139749550563, 'ou_sigma': 0.624467854316413, 'jrl_femur': -1.1045008457243952, 'jrl_tibia': -0.36821960835910844, 'jr_diff_femur': 1.018553462419685, 'jr_diff_tibia': 1.7157228930637012}.

# Add progress feature to obstacle

# QUAD TODO:
Make testing in sim with joystick (position driven)
Tune env to learn proper stable policy and learn policy again with transport delays (check christmas git to see what is different in version which used to work)
Make data gathering script

# TODO:
Make controllable policy on quad
Make and test data gathering on quad
Try RNN policy on obstacle
