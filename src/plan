Hexapod training:
# TODO: Add callback for id and NOTE reporting.
# TODO: Add callback for symmetry penalty using gradient descent
# TODO: Keep training straight and stairs policy
# TODO: Make checkpoint and best model copier from goedel
# TODO: Make proper input argument parser
# TODO: Custom callback for symmetry <-
# TODO: Make additional tensorboard logs to debug action, observation and gradient distributions during training

Hexapod HW:
# TODO: Make contact sensors
# TODO: Make contact + dampeners in CAD software
# TODO: Integrate T265 (both hardware and SW)
# TODO: Test how much energy each servo consumes in total and compare with simulation

Adaptation research:
# TODO: Try model learning on hangpole, including using RNN for adaptation
# TODO: Model based learning from a small amount of examples regularized by simulated examples
# TODO: Goal: Learn how to meta learn for supervised learning and RL
# TODO: Try meta learning (MAML and meta learning exploration) for supervised learning of transition probabilities

# TODO: Make evaluation of the trained regressor. Evaluation has to be accept any policy and regressor (add hidden state to rnn, btw).
# TODO: Evaluation also has to visibly show accuracy at every step.
# TODO: Make multiple policy and/or dropout training to gauge confidence.

# TODO: Questions to be answered
# TODO: Q1) Can you learn good model from random policy (how does it generalize to state distribution induced by trained policy)
# TODO: Q2) Does confidence using droupout or ensembles work
# TODO: Q3) Does RNN learn model for adapting param
# TODO: Q4) Using model for learning policy (later)

Research: Steps
# TODO: Prepare environments for training and testing. All environments must be in the same form as gym, but also sample-able upon reset
# TODO: Environments are for now the following: Cartpole, Cartpole swing up, Hexapod, Quadcopter stabilize, Quadcopter waypoints, Buggie, Airplane waypoints, Airplane land, Sailboat waypoint
# TODO: Make robotic platforms: Hexapod, Buggie, Quad and their respective simulations
# TODO: Test model learning for all envs. Is random exploration enough? How does a randomly trained model performance generalize to
# TODO: a state distribution induced by a policy learned by RL? Does learning using state visitation stratification help? Also compare to
# TODO: learning a model and a policy at the same time so that we are sampling from a relevant state distribution
# TODO: Can a model learning using RNN adapt to various parameter changes?
# TODO: Make a MAML and/or ANIL implementation which accepts our environments as input

Quad:
# TODO: Add feedforward reg to compensate yaw effect when pitching or rolling. Test Quad

Car HW:
# TODO: Start integrating hardware

Room pi:
# TODO: Install necessary stuff on pi
# TODO: Add and test night camera
# TODO: Install PI near bed

MAML:
# TODO: Fix quadruped env and for baseline and make sure PG works (test on pendulum and hexapod to see where problem is)
# TODO: Make PG into a class and make FULL DEBUG.
# TODO: Test MAML
# TODO: Make recurrent MAML
# TODO: Make CAVIA

# PG logging improvements:
# Add activations before and after non-lin

# VIR Projects:
1) Learning model of fast RC buggie using script which gathers data and using it to
   learn a forward function of the buggie using supervised learning.
2) Learning online stabilizing or flying for real quadcopter
3) Control of real buggie or quad using RL
5) Design RL rew and input features for driving a car around racetrack
6) Atlas entering and leaving car using RL
7) Hardware wireless sensor using neural nets (only for hw people)
8) Voice masking for anonymity

# TODO: Train hexapod using ES, td3 and a3c (longer time). After, add velocities and train again
# TODO: Add correct dirs to gitignore in pycharm
# TODO: Make apropriate scripts for deployment to goedel
# TODO: Put policies to train in goedel
# TODO: Add tb full logging to ES for linear policy