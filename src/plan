Hexapod:
# TODO: Add goal environment
# TODO: Tune training process for the whole policy suite
# TODO: Add difficulty curriculum to rough= and ->>> stairs envs<---
# TODO: Add joint range increase as difficulty increases to regularize the movements
# TODO: Fit contacts using simple function
# TODO: Penalize slippage
# TODO: Check that velocity is properly normalized across all target_velociteis
# TODO: See if you can fit simple reactive function to classify contact

Adaptation research:
# TODO: Make model based sensitivity propagation as advantage estimator
# TODO: Make RNN PG using new updated algo
# TODO: Fix up an adaptation env (hangpole probably) for adaptation testing
# TODO: Model based learning from a small amount of examples regularized by simulated examples
# TODO: Goal: Learn how to meta learn for supervised learning and RL
# TODO: Try meta learning (MAML and meta learning exploration) for supervised learning of transition probabilities

Paper:
# TODO: 1 per day

Quad:
# TODO: Add feedforward reg to compensate yaw effect when pitching or rolling. Test Quad

Hexapod HW:
# TODO: Test a learned policy on hexapod
# TODO: Integrate T265 (both hardware and SW)
# TODO: Test gamepad on hex through ros and through direct dongle (test range)
# TODO: Write full controll pipeline for hexapod (flat, rough and turn policies)
# TODO: Test how much energy each servo consumes in total and compare with simulation

Car HW:
# TODO: Start integrating hardware

