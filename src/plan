Adaptation research:

# TODO: Questions to be answered
# TODO: Q1) Can you learn good model from random policy (how does it generalize to state distribution induced by trained policy)
# TODO: Q2) Does confidence/uncertainty estimate using dropout or ensembles work
# TODO: Q3) Does RNN learn model for adapting param (supervised learning)

# Paper extract:
SNAIL:
- Gated activation functions: Might be usefull to allow or block output through
- Carrying over state to next episode apparently crucial. Needs more understanding
- Use of causal temporal convolutions alongside k,q,v attention
- Neural network architecture apparently important (skip connections, etc)
- Use larger batchsize (50k timesteps)

Learning VTOL policies:
- Integrated error input (similar as in PID) might be useful

RL^2:
- Apparently inter-episode hidden states matter

What matters in deep RL:
- Use the PPO policy loss. Start with the clipping threshold set to 0.25 but also try
lower and higher values if possible
- Initialize the last policy layer with 100× smaller weights
- Use tanh both as the activation function (if the networks are not too deep) and to transform the samples from the normal
distribution to the bounded action space. Use a wide value MLP (no layers shared with the policy)
but tune the policy width (it might need to be narrower than the value MLP).
- Recommendation. Use GAE with λ = 0.9 but neither Huber loss nor PPO-style value loss clipping.
- Discount factor γ is one of the most important hyperparameters and should be tuned per environment (start with γ = 0.99)
- Adam [8] optimizer with momentum β1 = 0.9 and a tuned learning rate (0.0003 is a safe default).
Linearly decaying the learning rate may slightly improve performance but is of secondary importance.
- One of the surprising insights is that the initial action distribution plays an important role in agent performance.

# Planned adaptation algos:
- Plain training using RNN (supervised model learning and RL policy)
- Oracle with params as inputs
- Using parameter predictor
- SNAIL - like architecture

# TODO:

# -----------
# TODO: Fix automation and optuna evaluation. (ADD SEED!!!)
# TODO: Find out how variance is handled on A2C SB
# TODO: INTEGRATE THE FUCKING CONV POLICY ON A2C SB
# TODO: THROW QUAD AND HEXA IN THE TUNER
# TODO: Check what can be run on GRID, perhaps it can be used for hyperparam opt
# -----------


# TODO: Make easy version of buggy env and check if SB and ES works on it. Then tune AC
# TODO: Reverse engineer A2C from SB3
# TODO: Keep working on AC!! We need a good working algo (parallel as well)
# TODO: Try hexapod version with MLP + IKT
# Find good hyperparams for both a2c and ac
# Study stable baselines3 policy modification through custom policy
# Start implementing adaptation algorithms. Start with temporal conv policy. (also add env augmentation for past obs)
# Put to train quadcopter and buggy envs with randomization


retrieve_ANC() {
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/PG/agents /home/silverjoda/SW/adaptive_neural_control/src/algos/PG
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/PG/tb /home/silverjoda/SW/adaptive_neural_control/src/algos/PG
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/SB/agents /home/silverjoda/SW/adaptive_neural_control/src/algos/SB
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/SB/tb /home/silverjoda/SW/adaptive_neural_control/src/algos/SB
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/ES/agents /home/silverjoda/SW/adaptive_neural_control/src/algos/ES
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/ES/tb /home/silverjoda/SW/adaptive_neural_control/src/algos/ES
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/AC/agents /home/silverjoda/SW/adaptive_neural_control/src/algos/AC
    rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/AC/tb /home/silverjoda/SW/adaptive_neural_control/src/algos/AC
    #rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/MAML/agents /home/silverjoda/PycharmProjects/adaptive_neural_control/src/algos/MAML
    #rsync -rP azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/algos/MAML/tb /home/silverjoda/PycharmProjects/adaptive_neural_control/src/algos/MAML
}

sync_ANC_goedel_clear() {
    python /home/silverjoda/SW/adaptive_neural_control/src/clear_all_data.py
    ssh azayetey@goedel.felk.cvut.cz 'python /home.dokt/azayetey/SW/adaptive_neural_control/src/clear_all_data.py && exit'
    rsync -rP /home/silverjoda/SW/adaptive_neural_control/src/ azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/
}

alias tblaunch='tensorboard --logdir .'
alias sync_ANC_goedel='rsync -rP /home/silverjoda/SW/adaptive_neural_control/src/ azayetey@goedel.felk.cvut.cz:/home.dokt/azayetey/SW/adaptive_neural_control/src/'
alias sshgoedel='ssh azayetey@goedel.felk.cvut.cz'
alias sshpi='ssh -X pi@raspberrypi.local'
alias sshpiz='ssh pi@piz.local'
alias dimscreen='xrandr --output HDMI-0 --brightness 0.5'
