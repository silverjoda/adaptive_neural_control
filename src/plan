Adaptation research:

# TODO: Questions to be answered
# TODO: Q1) Can you learn good model from random policy (how does it generalize to state distribution induced by trained policy)
# TODO: Q2) Does confidence/uncertainty estimate using dropout or ensembles work
# TODO: Q3) Does RNN learn model for adapting param (supervised learning)



# Paper extract:
SNAIL:
- Gated activation functions: Might be usefull to allow or block output through
- Carrying over state to next episode apparently crucial. Needs more understanding
- Use of causal temporal convolutions alongside k,q,v attention
- Neural network architecture apparently important (skip connections, etc)
- Use larger batchsize (50k timesteps)

Learning VTOL policies:
- Integrated error input (similar as in PID) might be useful

RL^2:
- Apparently inter-episode hidden states matter

What matters in deep RL:
- Use the PPO policy loss. Start with the clipping threshold set to 0.25 but also try
lower and higher values if possible
- Initialize the last policy layer with 100× smaller weights
- Use tanh both as the activation function (if the networks are not too deep) and to transform the samples from the normal
distribution to the bounded action space. Use a wide value MLP (no layers shared with the policy)
but tune the policy width (it might need to be narrower than the value MLP).
- Recommendation. Use GAE with λ = 0.9 but neither Huber loss nor PPO-style value loss clipping.
- Discount factor γ is one of the most important hyperparameters and should be tuned per environment (start with γ = 0.99)
- Adam [8] optimizer with momentum β1 = 0.9 and a tuned learning rate (0.0003 is a safe default).
Linearly decaying the learning rate may slightly improve performance but is of secondary importance.
- One of the surprising insights is that the initial action distribution plays an important role in agent performance.

# Planned revision for environments:
- Add transport delay
- Check randomized parameter generation for each env. Decide on which to keep (test all envs to find proper range)
- Add param to dict output and conditionally to agent input (conditioned by boolean in config)

# Planned adaptation algos:
- Plain training using RNN (supervised model learning and RL policy)
- Oracle with params as inputs
- Using parameter predictor
- SNAIL - like architecture

# TODO:
# Test if quadcopter done condition on just pos_delta is ok.
# Start implementing adaptation algorithms. Start with temporal conv policy. (also add env augmentation for past obs)
# Put to train quadcopter and buggy envs with randomization
