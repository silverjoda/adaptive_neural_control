Hexapod:
# TODO: Add goal environment
# TODO: Tune training process for the whole policy suite
# TODO: Try learning leg model in air using random policy and see if the model regression is as accurate under a different policy (non-random)
# TODO: Try to learn contact model using leg model

Adaptation research:
# TODO: Make model based sensitivity propagation as advantage estimator
# TODO: Make RNN PG using new updated algo
# TODO: Fix up an adaptation env (hangpole probably) for adaptation testing
# TODO: Model based learning from a small amount of examples regularized by simulated examples
# TODO: Goal: Learn how to meta learn for supervised learning and RL
# TODO: Try meta learning (MAML and meta learning exploration) for supervised learning of transition probabilities

Research: Steps
# TODO: Prepare environments for training and testing. All environments must be in the same form as gym, but also sample-able upon reset
# TODO: Environments are utimately the following: Cartpole, Cartpole swing up, Hexapod, Quadcopter stabilize, Quadcopter waypoints, Buggie, Airplane waypoints, Airplane land, Sailboat waypoint
# TODO: Test model learning for all envs. Is random exploration enough? How does a randomly trained model performance generalize to
# TODO: a state distribution induced by a policy learned by RL? Does learning using state visitation stratification help? Also compare to
# TODO: learning a model and a policy at the same time so that we are sampling from a relevant state distribution
# TODO: Can a model learning using RNN adapt to various parameter changes?
# TODO: Make a MAML and/or ANIL implementation which accepts our environments as input

Paper:
# TODO: 1 per day

Quad:
# TODO: Add feedforward reg to compensate yaw effect when pitching or rolling. Test Quad

Hexapod HW:
# TODO: Test learned policies on hexapod
# TODO: Make contact sensors
# TODO: Integrate T265 (both hardware and SW)
# TODO: Test how much energy each servo consumes in total and compare with simulation

Car HW:
# TODO: Start integrating hardware

# VIR Projects:
1) Learning model of fast RC buggie using script which gathers data and using it to
   learn a forward function of the buggie using supervised learning.
2) Learning online stabilizing or flying for real quadcopter
3) Control of real car or quad using RL
4) Model based control of a KUKA robot using gradient descent
