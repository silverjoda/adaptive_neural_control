Hexapod training:
# TODO: Add callback for id and NOTE reporting.
# TODO: Add callback for symmetry penalty using gradient descent
# TODO: Keep training straight and stairs policy
# TODO: Make checkpoint and best model copier from goedel
# TODO: Make proper input argument parser
# TODO: Custom callback for symmetry <-
# TODO: Make additional tensorboard logs to debug action, observation and gradient distributions during training

Hexapod HW:
# TODO: Make contact sensors
# TODO: Make contact + dampeners in CAD software
# TODO: Integrate T265 (both hardware and SW)
# TODO: Test how much energy each servo consumes in total and compare with simulation

Adaptation research:
# TODO: Try model learning on hangpole, including using RNN for adaptation
# TODO: Model based learning from a small amount of examples regularized by simulated examples
# TODO: Goal: Learn how to meta learn for supervised learning and RL
# TODO: Try meta learning (MAML and meta learning exploration) for supervised learning of transition probabilities

# TODO: Make evaluation of the trained regressor. Evaluation has to be accept any policy and regressor (add hidden state to rnn, btw).
# TODO: Evaluation also has to visibly show accuracy at every step.
# TODO: Make multiple policy and/or dropout training to gauge confidence.

# TODO: Questions to be answered
# TODO: Q1) Can you learn good model from random policy (how does it generalize to state distribution induced by trained policy)
# TODO: Q2) Does confidence using droupout or ensembles work
# TODO: Q3) Does RNN learn model for adapting param
# TODO: Q4) Using model for learning policy (later)

Research: Steps
# TODO: Prepare environments for training and testing. All environments must be in the same form as gym, but also sample-able upon reset
# TODO: Environments are for now the following: Cartpole, Cartpole swing up, Hexapod, Quadcopter stabilize, Quadcopter waypoints, Buggie, Airplane waypoints, Airplane land, Sailboat waypoint
# TODO: Make robotic platforms: Hexapod, Buggie, Quad and their respective simulations
# TODO: Test model learning for all envs. Is random exploration enough? How does a randomly trained model performance generalize to
# TODO: a state distribution induced by a policy learned by RL? Does learning using state visitation stratification help? Also compare to
# TODO: learning a model and a policy at the same time so that we are sampling from a relevant state distribution
# TODO: Can a model learning using RNN adapt to various parameter changes?
# TODO: Make a MAML and/or ANIL implementation which accepts our environments as input

Quad:
# TODO: Add feedforward reg to compensate yaw effect when pitching or rolling. Test Quad

Car HW:
# TODO: Start integrating hardware

Room pi:
# TODO: Install necessary stuff on pi
# TODO: Add and test night camera
# TODO: Install PI near bed

MAML:
# TODO: Make PG into a class and make FULL DEBUG.
# TODO: Test MAML
# TODO: Make recurrent MAML
# TODO: Make CAVIA

# PG logging improvements:
# Add activations before and after non-lin

# VIR Projects:
1) Learning model of fast RC buggie using script which gathers data and using it to
   learn a forward function of the buggie using supervised learning. The state space should then be plotted using TSNE or something similar.
   Outliers can be ommited from dataset and model retrained to see if it can generalize to them.
6) Atlas entering and leaving car using RL
7) Hardware wireless sensor using neural nets (only for hw people)
8) Voice masking for anonymity
9) Make racecar in simulation drive fast around track using RL. Compare with human performance.

# TODO: Try PG with various hyperparams to se which such
# TODO: Check PG again with full tb
# TODO: Check MAML with full gradients (Also try hyperparams from initial paper)
# TODO: Try ES MAML
# TODO: MAML for model training
# TODO: Continue training hexapod

# Video explaining learning:
- Using a programmed approach vs learning approach for a general problem
- Creating a suitable solution space (either directly or through a function approximator)
- Function approximation, universality and turing completeness
- Compositionality of tasks, why a multi layer NN is necessary
- 5 images classification with pixel in the top corner
- Priors, from LUT to hard-priored function
- Optimization of solution, brute force & random search

# GOEDEL:
# Poor policy performance might have been due to parameter changes, etc. Running all envs again

Hexapod: Currently training last test /w velocity and trying w/ and w/o footpenalty (last experiment, before going waypoint). 1) is w/ fp, 2) is w/0
TODO: HW: Go straight for waypoint training flat and rough. Add velocity to observation,
try randomization (if doesn't work then try without reloading the env, only use programmatic changes). Also
adjust operating point.
# TODO: Try without tip velocity penalty

Buggy: Works ok. TODO: Test if second target does anything

Quadcopter: TODO: Works ok, should try input speed training next

# TODO: Install stable baselines on robots so that we can use policy directly
# TODO: Start writing data processing and learning scripts for all data
# TODO: Start writing Adaptation learning algorithms
# TODO: Try out a gradient based evolution algorithm (from scratch)
# TODO: Perhaps maximally informative trajectories can improve MAML/CAVIA
# TODO: Test reactive vs RNN policies on randomized buggy or quad env (Try various rnn architectures)

# Paper extract:
SNAIL:
- Gated activation functions: Might be usefull to allow or block output through
- Carrying over state to next episode apparently crucial. Needs more understanding
- Use of causal temporal convolutions alongside k,q,v attention
- Neural network architecture apparently important (skip connections, etc)
- Use larger batchsize (50k timesteps)