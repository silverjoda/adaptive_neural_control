Adaptation research:

# TODO: Questions to be answered
# TODO: Q1) Can you learn good model from random policy (how does it generalize to state distribution induced by trained policy)
# TODO: Q2) Does confidence/uncertainty estimate using dropout or ensembles work
# TODO: Q3) Does RNN learn model for adapting param (supervised learning)

# Paper extract:
SNAIL:
- Gated activation functions: Might be usefull to allow or block output through
- Carrying over state to next episode apparently crucial. Needs more understanding
- Use of causal temporal convolutions alongside k,q,v attention
- Neural network architecture apparently important (skip connections, etc)
- Use larger batchsize (50k timesteps)

Learning VTOL policies:
- Integrated error input (similar as in PID) might be useful

RL^2:
- Apparently inter-episode hidden states matter

What matters in deep RL:
- Use the PPO policy loss. Start with the clipping threshold set to 0.25 but also try
lower and higher values if possible
- Initialize the last policy layer with 100× smaller weights
- Use tanh both as the activation function (if the networks are not too deep) and to transform the samples from the normal
distribution to the bounded action space. Use a wide value MLP (no layers shared with the policy)
but tune the policy width (it might need to be narrower than the value MLP).
- Recommendation. Use GAE with λ = 0.9 but neither Huber loss nor PPO-style value loss clipping.
- Discount factor γ is one of the most important hyperparameters and should be tuned per environment (start with γ = 0.99)
- Adam [8] optimizer with momentum β1 = 0.9 and a tuned learning rate (0.0003 is a safe default).
Linearly decaying the learning rate may slightly improve performance but is of secondary importance.
- One of the surprising insights is that the initial action distribution plays an important role in agent performance.

# Planned adaptation algos:
- Plain training using RNN (supervised model learning and RL policy)
- Oracle with params as inputs
- Using parameter predictor
- SNAIL - like architecture

# TODO:

# -----------
# TODO: Currently integrating conv policy
# TODO: Somethign wrong with quad env, not training
# TODO: Find out how variance is handled on A2C SB
# TODO: THROW QUAD AND HEXA IN THE TUNER
# TODO: Check what can be run on GRID, perhaps it can be used for hyperparam opt
# -----------


# TODO: Make easy version of buggy env and check if SB and ES works on it. Then tune AC
# TODO: Reverse engineer A2C from SB3
# TODO: Keep working on AC!! We need a good working algo (parallel as well)
# TODO: Try hexapod version with MLP + IKT
# Find good hyperparams for both a2c and ac
# Study stable baselines3 policy modification through custom policy
# Start implementing adaptation algorithms. Start with temporal conv policy. (also add env augmentation for past obs)
# Put to train quadcopter and buggy envs with randomization

# TODO: Hexa works on flat with turning perfectly. Now try on perlin, with additional NN inputs (correction from NN)
Best params:  {'n_steps': 38, 'learning_rate': 0.0004041431413252437, 'gamma': 0.9970081288014816, 'p
hase_scalar': 0.26881373950773213, 'phase_decay': 0.988248774110033}  Best value:  43.675244140625


# On RCI cluster:
ml Python/3.7.4-GCCcore-8.3.0 # To load python

# Python installs:
pip install --user optuna
pip install --user stable_baselines3
pip install --user opensimplex
pip install --user pybullet


# Schedule

