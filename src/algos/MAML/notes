# TODO: Idea 1: Try RL MAML with recurrent policy and compare performance with normal (task=quadruped with various leg lengths)
# TODO: Idea 2: Try RL MAML with reactive policy + recurrent exploration policy. Exploration policy is trained using a) RL with a numerical rew at the end
# TODO: of the episode b) Using gradients an RNN predictor of the numerical rew at the end of the episode (which itself is learned using supervised learning)
# TODO: Idea 3: Compare model learning performance using the following: gaussian noise, simplex noise, learned RNN noise which maximizes error of model (adversarial), RNN noise which maximizes state visitation using hash dicts, concurrently learning RL policy.
# TODO: Question 1: On such a learned model (Idea 3) using non-adversarial noise, can we learn an adversarial policy which significantly impacts performance of model predictor.
# TODO: Question 2: Can any of the models from idea 3 be used for something useful? Such as planning, accelerated learning (credit assignment using backprop), or imagination?
# TODO: Idea 4: Try MAML learn a model using gaussian, simplex noise concurrently learned adversarial policy and state visitation maximization policy.
